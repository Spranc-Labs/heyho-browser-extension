# Phase 2.1: Race Conditions and Reliability Fixes

## Overview

Following the implementation of the two-tiered aggregation system (Phase 2), several critical race conditions and reliability issues have been identified that could lead to data corruption, loss, or inconsistent state. This specification addresses these issues with prioritized solutions.

## Critical Issues Identified

### ðŸš¨ Priority 1: Immediate Fixes Required

#### 1. Concurrent Aggregation Prevention
**Issue:** Multiple aggregation processes can run simultaneously, causing data corruption.
```javascript
// Problem: No protection against concurrent execution
async function processEvents() {
  const events = await getAllEvents(); // Could read same events twice
  // ... process events ...
  await executeTransaction(operations); // Could conflict with parallel process
}
```

**Impact:** 
- Duplicate processing of events
- Race conditions in database transactions
- Data corruption in tabAggregates and pageVisits

**Solution:** Implement processing lock mechanism
```javascript
let isProcessing = false;
const PROCESSING_TIMEOUT = 300000; // 5 minutes max

async function processEvents() {
  if (isProcessing) {
    console.log('Aggregation already in progress, skipping...');
    return { success: false, reason: 'already_processing' };
  }
  
  isProcessing = true;
  const startTime = Date.now();
  
  try {
    // Add timeout protection
    const timeoutPromise = new Promise((_, reject) => 
      setTimeout(() => reject(new Error('Processing timeout')), PROCESSING_TIMEOUT)
    );
    
    await Promise.race([actualProcessingLogic(), timeoutPromise]);
    
    return { success: true, duration: Date.now() - startTime };
  } finally {
    isProcessing = false;
  }
}
```

#### 2. Active Visit State Atomicity
**Issue:** activeVisit state can be corrupted by concurrent reads/writes.

**Solution:** Make activeVisit updates atomic with database operations
```javascript
// Include activeVisit in the same transaction as other operations
operations.push({
  type: 'setActiveVisit',
  data: currentActiveVisit
});
```

### âš ï¸ Priority 2: Data Integrity Fixes

#### 3. Transaction Failure Recovery
**Issue:** If `executeTransaction` fails after events are marked for deletion, data is lost with no recovery.

**Solution:** Implement two-phase commit pattern
```javascript
async function processEvents() {
  const events = await getAllEvents();
  const operations = [];
  const eventIds = [];
  
  // Phase 1: Process and prepare operations (no deletions yet)
  for (const event of events) {
    processEvent(event, activeVisit, operations, tabAggregatesMap);
    eventIds.push(event.id);
  }
  
  // Phase 2: Execute operations first, then delete events only on success
  try {
    await executeTransaction(operations);
    // Only delete events if operations succeeded
    await deleteEvents(eventIds);
  } catch (error) {
    // Events remain for retry, no data loss
    throw error;
  }
}
```

#### 4. Large Dataset Memory Protection
**Issue:** Loading all tabAggregates and events into memory could crash extension.

**Solution:** Implement batch processing with limits
```javascript
const MAX_EVENTS_PER_BATCH = 1000;
const MAX_AGGREGATES_PER_BATCH = 500;

async function processEventsBatched() {
  const eventCount = await getEventsCount();
  
  if (eventCount > MAX_EVENTS_PER_BATCH) {
    // Process in smaller batches
    for (let offset = 0; offset < eventCount; offset += MAX_EVENTS_PER_BATCH) {
      await processBatch(offset, MAX_EVENTS_PER_BATCH);
    }
  } else {
    await processEvents();
  }
}
```

### ðŸ“Š Priority 3: Performance and Monitoring

#### 5. Events Lost During Processing
**Issue:** New events arriving during aggregation are missed until next cycle.

**Solution:** Add event sequence tracking
```javascript
// Add sequence numbers to events
function createCoreEvent(type, tabId, url) {
  return {
    id: generateEventId(),
    sequence: getNextSequenceNumber(), // New field
    timestamp: Date.now(),
    type,
    tabId,
    url,
    domain: extractDomain(url)
  };
}

// Detect missed events
async function checkForMissedEvents(lastProcessedSequence) {
  const gaps = await findSequenceGaps(lastProcessedSequence);
  if (gaps.length > 0) {
    console.warn(`Detected ${gaps.length} missed event sequences:`, gaps);
    // Trigger recovery process
  }
}
```

#### 6. Service Worker Restart Recovery
**Issue:** In-memory state lost when service worker terminates.

**Solution:** Persist processing state
```javascript
// Store processing state in chrome.storage.local
const PROCESSING_STATE_KEY = 'aggregation_state';

async function saveProcessingState(state) {
  await chrome.storage.local.set({
    [PROCESSING_STATE_KEY]: {
      ...state,
      timestamp: Date.now()
    }
  });
}

async function recoverFromRestart() {
  const state = await chrome.storage.local.get(PROCESSING_STATE_KEY);
  const processingState = state[PROCESSING_STATE_KEY];
  
  if (processingState && isStaleProcessingState(processingState)) {
    console.log('Detected stale processing state, cleaning up...');
    await chrome.storage.local.remove(PROCESSING_STATE_KEY);
    isProcessing = false; // Reset processing lock
  }
}
```

## Implementation Plan

### Phase 2.1.1: Critical Fixes (Week 1)
- [ ] Implement processing lock mechanism
- [ ] Add activeVisit atomicity 
- [ ] Add transaction failure recovery
- [ ] Add basic memory limits

### Phase 2.1.2: Reliability Improvements (Week 2)
- [ ] Implement event sequence tracking
- [ ] Add service worker restart recovery
- [ ] Add processing timeout protection
- [ ] Implement batch processing for large datasets

### Phase 2.1.3: Monitoring and Observability (Week 3)
- [ ] Add processing metrics and health checks
- [ ] Implement error reporting and recovery stats
- [ ] Add data consistency validation
- [ ] Create debugging tools for race condition detection

## Testing Strategy

### Unit Tests
```javascript
describe('Race Condition Prevention', () => {
  test('concurrent aggregation attempts should be rejected', async () => {
    const promise1 = processEvents();
    const promise2 = processEvents();
    
    const results = await Promise.all([promise1, promise2]);
    
    expect(results.filter(r => r.success).length).toBe(1);
    expect(results.filter(r => r.reason === 'already_processing').length).toBe(1);
  });
});
```

### Integration Tests
```javascript
describe('Data Consistency', () => {
  test('transaction failure should not lose events', async () => {
    // Mock transaction failure
    StorageModule.executeTransaction.mockRejectedValueOnce(new Error('DB error'));
    
    const eventsBefore = await getAllEvents();
    await expect(processEvents()).rejects.toThrow();
    const eventsAfter = await getAllEvents();
    
    expect(eventsAfter.length).toBe(eventsBefore.length); // No events lost
  });
});
```

## Success Metrics

- **Zero data loss** during aggregation failures
- **No duplicate processing** of events
- **Processing lock effectiveness** - 100% prevention of concurrent runs
- **Recovery success rate** - 95%+ successful recovery from failures
- **Performance** - No memory crashes with large datasets
- **Consistency** - Aggregated data always matches raw events

## Risk Assessment

| Risk | Probability | Impact | Mitigation |
|------|-------------|---------|------------|
| Concurrent processing corruption | High | Critical | Processing lock (2.1.1) |
| Data loss on transaction failure | Medium | Critical | Two-phase commit (2.1.1) |
| Memory crashes with large datasets | Medium | High | Batch processing (2.1.2) |
| Service worker restart data loss | Low | Medium | State persistence (2.1.2) |

## Dependencies

- Phase 2 aggregation system must be fully implemented
- No breaking changes to existing APIs
- Backward compatibility with existing data

## Rollback Plan

All changes should be implemented with feature flags to enable quick rollback:
```javascript
const ENABLE_PROCESSING_LOCK = true;
const ENABLE_BATCH_PROCESSING = true;
const ENABLE_SEQUENCE_TRACKING = false; // Can be enabled gradually
```

---

**Next Phase:** Phase 3 - Advanced Analytics and Insights Engine